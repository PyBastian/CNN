{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile as zf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import filecmp as fcmp\n",
        "import os\n",
        "import scipy as sp\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import gradient_descent_v2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import History, EarlyStopping\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import cv2 as cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Dense, MaxPooling2D, Conv2D, Flatten, Dropout\n",
        "from keras import regularizers\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sbn\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, train_test_split\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm\n",
        "from keras import losses\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import warnings\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import PIL.Image\n",
        "import itertools\n",
        "from keras_tuner import RandomSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "FxhbePQWwWUb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_dataset = pd.read_csv(\"train.csv\")\n",
        "test_dataset = pd.read_csv(\"test.csv\")\n",
        "\n",
        "\n",
        "data_dir  = \"\"\n",
        "train_zip_path = data_dir + 'Train'\n",
        "test_zip_path = data_dir + 'Test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "oe0s2wSCx9RP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'0': 'Anfibio', '1': 'Artropodo', '2': 'Ave', '3': 'Mamifero', '4': 'Reptil', '5': 'Rana', '6': 'Salamandra', '7': 'Sapo', '8': 'Lombriz', '9': 'Mariposa', '10': 'Saltamontes', '11': 'Tarantula', '12': 'Viuda negra', '13': 'Buho', '14': 'Cacique', '15': 'Cisne', '16': 'Cormoran', '17': 'Ganso', '18': 'Paloma', '19': 'Pato', '20': 'Pavo', '21': 'Caballo', '22': 'Cabra', '23': 'Conejo', '24': 'Elefante', '25': 'Gato', '26': 'Leon', '27': 'Lobo', '28': 'Oso', '29': 'Perro', '30': 'Tigre', '31': 'Camaleon', '32': 'Cocodrilo', '33': 'Iguana', '34': 'Lagarto', '35': 'Serpiente', '36': 'Tortuga'}\n"
          ]
        }
      ],
      "source": [
        "dicts = np.load(\"dict.npy\",allow_pickle='TRUE')\n",
        "print (dicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "shapes = []\n",
        "for dirname, _, filenames in os.walk(\"Train\"):           \n",
        "    for filename in filenames:\n",
        "        try:\n",
        "            path = os.path.join(dirname, filename)\n",
        "            image = Image.open(path)\n",
        "            if image.size not in shapes:\n",
        "                shapes.append(image.size)\n",
        "        except:\n",
        "            pass\n",
        "#print(shapes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "#si es necesario\n",
        "#!pip install scikit-image\n",
        "import skimage.transform\n",
        "\n",
        "# you use this just before passing any image to a CNN\n",
        "# which usually expects square images\n",
        "# however your input images can be of variable size\n",
        "# you don't want to just squash the images to a square\n",
        "# because you will lose valuable aspect ratio information\n",
        "# you want to resize while preserving the aspect ratio\n",
        "# these 2 functions perform this resizing behaviour\n",
        "# images are assumed to be formatted as Height, Width, Channels\n",
        "# we will use bound_image_dim to first bound the image to a range\n",
        "# the smallest dimension will be scaled up to the min_size\n",
        "# the largest dimension will be scaled down to the max_size\n",
        "# then afterwards you square_pad_image to pad the image to a square\n",
        "# filling the empty space with zeros\n",
        "\n",
        "def bound_image_dim(image, min_size=None, max_size=None):\n",
        "    if (max_size is not None) and \\\n",
        "       (min_size is not None) and \\\n",
        "       (max_size < min_size):\n",
        "        raise ValueError('`max_size` must be >= to `min_size`')\n",
        "    dtype = image.dtype\n",
        "    (height, width, *_) = image.shape\n",
        "    # scale the same for both height and width for fixed aspect ratio resize\n",
        "    scale = 1\n",
        "    # bound the smallest dimension to the min_size\n",
        "    if min_size is not None:\n",
        "        image_min = min(height, width)\n",
        "        scale = max(1, min_size / image_min)\n",
        "    # next, bound the largest dimension to the max_size\n",
        "    # this must be done after bounding to the min_size\n",
        "    if max_size is not None:\n",
        "        image_max = max(height, width)\n",
        "        if round(image_max * scale) > max_size:\n",
        "            scale = max_size / image_max\n",
        "    if scale != 1:\n",
        "        image = skimage.transform.resize(\n",
        "            image, (round(height * scale), round(width * scale)),\n",
        "            order=1,\n",
        "            mode='constant',\n",
        "            preserve_range=True)\n",
        "    return image.astype(dtype)\n",
        "\n",
        "def square_pad_image(image, size):\n",
        "    (height, width, *_) = image.shape\n",
        "    if (size < height) or (size < width):\n",
        "        raise ValueError('`size` must be >= to image height and image width')\n",
        "    pad_height = (size - height) / 2\n",
        "    pad_top = math.floor(pad_height)\n",
        "    pad_bot = math.ceil(pad_height)\n",
        "    pad_width = (size - width) / 2\n",
        "    pad_left = math.floor(pad_width)\n",
        "    pad_right = math.ceil(pad_width)\n",
        "    return np.pad(\n",
        "        image, ((pad_top, pad_bot), (pad_left, pad_right), (0, 0)),\n",
        "        mode='constant')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = []\n",
        "HEIGHT = 32\n",
        "WIDTH = 55\n",
        "# loop over the input images\n",
        "for index, imagePath in train_dataset.iterrows():\n",
        "    # load the image, resize the image to be HEIGHT * WIDTH pixels (ignoring\n",
        "    # aspect ratio) and store the image in the data list\n",
        "    try:\n",
        "        image = cv2.imread('//'+train_zip_path+'//'+imagePath[0])\n",
        "        image = cv2.resize(image, (HEIGHT, WIDTH))\n",
        "        print(image, imagePath[0])\n",
        "        data.append(image)\n",
        "    except:\n",
        "        #print(\"failed\", imagePath[0])\n",
        "        continue\n",
        "    print(type(image))\n",
        "    \n",
        "    # extract the class label from the image path and update the\n",
        "    # labels list\n",
        "#print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image_100.jpg\n",
            "image_109.jpg\n",
            "image_12.jpg\n",
            "image_1212.jpg\n",
            "image_1220.jpg\n",
            "image_1221.jpg\n",
            "image_1222.jpg\n",
            "image_1228.jpg\n",
            "image_14.jpg\n",
            "image_17.jpg\n",
            "image_19.jpg\n",
            "image_23.jpg\n",
            "image_26.jpg\n",
            "image_29.jpg\n",
            "image_32.jpg\n",
            "image_4305.jpg\n",
            "image_4307.jpg\n",
            "image_4310.jpg\n",
            "image_4312.jpg\n",
            "image_4337.jpg\n",
            "image_4338.jpg\n",
            "image_4342.jpg\n",
            "image_4346.jpg\n",
            "image_61.jpg\n",
            "image_69.jpg\n",
            "image_70.jpg\n",
            "image_73.jpg\n",
            "image_79.jpg\n",
            "image_8.jpg\n",
            "image_809.jpg\n",
            "image_810.jpg\n",
            "image_811.jpg\n",
            "image_828.jpg\n",
            "image_829.jpg\n",
            "image_831.jpg\n",
            "image_833.jpg\n",
            "image_838.jpg\n",
            "image_842.jpg\n",
            "image_843.jpg\n",
            "image_856.jpg\n",
            "image_857.jpg\n",
            "image_871.jpg\n",
            "image_877.jpg\n",
            "image_879.jpg\n",
            "image_881.jpg\n",
            "image_888.jpg\n",
            "image_889.jpg\n",
            "image_95.jpg\n",
            "image_96.jpg\n",
            "image_99.jpg\n",
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ],
      "source": [
        "images = list()\n",
        "labels = list()\n",
        "\n",
        "for dirname, _, filenames in os.walk(test_zip_path):           \n",
        "    for filename in filenames:\n",
        "        try:\n",
        "            image = Image.open(os.path.join(dirname, filename))\n",
        "            images.append(np.array(image, dtype=np.uint8))\n",
        "            print(filename)\n",
        "            print(train_dataset.loc[train_dataset['Id'] == filename.strip()].iloc[0][1])\n",
        "            \n",
        "            labels.append(train_dataset.loc[train_dataset['Id'] == filename].iloc[0][1])\n",
        "        except:\n",
        "            pass\n",
        "images = np.asarray(images)\n",
        "labels = np.asarray(labels)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0 5'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.loc[train_dataset['Id'] == 'image_1.jpg'].iloc[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[0.96470588, 0.95686275, 0.89803922],\n",
              "        [0.96470588, 0.95686275, 0.89803922],\n",
              "        [0.96470588, 0.96078431, 0.89019608],\n",
              "        ...,\n",
              "        [0.07843137, 0.07058824, 0.0745098 ],\n",
              "        [0.07843137, 0.07058824, 0.0745098 ],\n",
              "        [0.07843137, 0.07058824, 0.0745098 ]],\n",
              "\n",
              "       [[0.96862745, 0.96078431, 0.90196078],\n",
              "        [0.96862745, 0.96078431, 0.90196078],\n",
              "        [0.96862745, 0.96470588, 0.89411765],\n",
              "        ...,\n",
              "        [0.0627451 , 0.05490196, 0.05882353],\n",
              "        [0.0627451 , 0.05490196, 0.05882353],\n",
              "        [0.0627451 , 0.05490196, 0.05882353]],\n",
              "\n",
              "       [[0.97254902, 0.96470588, 0.90588235],\n",
              "        [0.97254902, 0.96470588, 0.90588235],\n",
              "        [0.97254902, 0.96862745, 0.89803922],\n",
              "        ...,\n",
              "        [0.0627451 , 0.05490196, 0.05882353],\n",
              "        [0.05882353, 0.05098039, 0.05490196],\n",
              "        [0.05882353, 0.05098039, 0.05490196]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.07058824, 0.07058824, 0.07058824],\n",
              "        [0.08627451, 0.08627451, 0.08627451],\n",
              "        [0.09411765, 0.09411765, 0.09411765],\n",
              "        ...,\n",
              "        [0.07843137, 0.04313725, 0.        ],\n",
              "        [0.18039216, 0.14117647, 0.03529412],\n",
              "        [0.49411765, 0.45098039, 0.3254902 ]],\n",
              "\n",
              "       [[0.0627451 , 0.0627451 , 0.0627451 ],\n",
              "        [0.08235294, 0.08235294, 0.08235294],\n",
              "        [0.09019608, 0.09019608, 0.09019608],\n",
              "        ...,\n",
              "        [0.1254902 , 0.08627451, 0.04705882],\n",
              "        [0.11764706, 0.0745098 , 0.00392157],\n",
              "        [0.25882353, 0.21176471, 0.1254902 ]],\n",
              "\n",
              "       [[0.05882353, 0.05882353, 0.05882353],\n",
              "        [0.07843137, 0.07843137, 0.07843137],\n",
              "        [0.09019608, 0.09019608, 0.09019608],\n",
              "        ...,\n",
              "        [0.1372549 , 0.09803922, 0.0627451 ],\n",
              "        [0.11372549, 0.06666667, 0.01176471],\n",
              "        [0.14901961, 0.09803922, 0.03137255]]])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images = images/255\n",
        "images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "labelencode = preprocessing.LabelEncoder()\n",
        "\n",
        "labelencode.fit(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((40,), (10,), (40,), (10,))"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.20, random_state=10)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same', input_shape=(224,224,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dropout(rate=0.7))\n",
        "model.add(Dense(units=3, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 224, 224, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 112, 112, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 54, 54, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 27, 27, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 46656)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               5972096   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 5,996,067\n",
            "Trainable params: 5,996,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: '0 5'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-65-2600c222cef4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m   \"\"\"\n\u001b[1;32m---> 66\u001b[1;33m   \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '0 5'"
          ]
        }
      ],
      "source": [
        "history = model.fit(x=x_train, y=to_categorical(y_train), epochs=6, validation_data=(x_test, to_categorical(y_test)), batch_size=20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "9073859de2683ea83eaf7a4e8b63cb105e54ed0d903e4a97f6d49ddec64b0227"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
